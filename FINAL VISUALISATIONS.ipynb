{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from database.connect import getConnection\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from processes.reply4_3 import reply_set_up, split_df, welch_test\n",
    "import statistics\n",
    "from sklearn import linear_model\n",
    "import database.connect\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CLEAR OUTPUTS BEFORE PUSHING TO GIT**\n",
    "Press the clear all outputs button in the toolbar above before pushing to git.  \n",
    "This makes version control easier and avoids merge conflicts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '01-05-2010'\n",
    "end_date = '01-06-2023'\n",
    "\n",
    "# convert start and end date to unix timestamp in milliseconds\n",
    "start_date_unix = int(datetime.strptime(start_date, '%d-%m-%Y').timestamp() * 1000)\n",
    "end_date_unix = int(datetime.strptime(end_date, '%d-%m-%Y').timestamp() * 1000)\n",
    "\n",
    "# create a part you can insert into your where clause\n",
    "date_restriction = f\" AND tweets.timestamp_ms >= {start_date_unix} AND tweets.timestamp_ms <= {end_date_unix}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below creates the connection to the database and the cursor to execute queries.  \n",
    "It also contains variables relevant everywhere in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    connection = getConnection()\n",
    "except Exception:\n",
    "    print(\"✖️ Error while connecting to MySQL engine database.\")\n",
    "    print(\"ℹ️ Please make sure the environment file `.env` is located at\"+\n",
    "        \"the project root directory and contains proper configuration.\")\n",
    "    raise\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "categories = {'booking': ['booking', 'booked', 'book', 'ticket', 'tickets'],\n",
    "            'canceling': ['canceled', 'cancellations'],\n",
    "            'money': ['refund', 'compensation', 'claim', 'money', 'pay', 'paid'],\n",
    "            'baggage': ['bag', 'baggage', 'luggage', 'bags'],\n",
    "            'staff': ['staff', 'crew'],\n",
    "            'waiting': ['waiting', 'delay', 'wait'],\n",
    "            'boarding': ['boarding'],\n",
    "            'stuck': ['stuck'],\n",
    "\n",
    "            'information': ['info', 'information'],\n",
    "            'customers': ['customer', 'customers', 'passenger', 'passengers'],\n",
    "            'dm': [' dm'], \n",
    "            }\n",
    "\n",
    "airlines_dict = {'KLM': ['klm'],\n",
    "                'AirFrance':['airfrance',\n",
    "                            'air france'],\n",
    "                'British_Airways': ['british_airways',\n",
    "                                    'british airways'],\n",
    "                'AmericanAir': ['americanair',\n",
    "                                'american airlines'],\n",
    "                'Lufthansa': ['lufthansa'],\n",
    "                'AirBerlin': ['airberlin',\n",
    "                                'air berlin'],\n",
    "                'AirBerlin assist': ['airberlin assist',\n",
    "                                    'air berlin assist',\n",
    "                                    'airberlinassist'],\n",
    "                'easyJet': ['easyjet'],\n",
    "                'RyanAir': ['ryanair'],\n",
    "                'SingaporeAir': ['singaporeair',\n",
    "                                'singapore airlines'],\n",
    "                'Qantas': ['qantas'],\n",
    "                'EtihadAirways': ['etihad airways',\n",
    "                                'etihadairways',\n",
    "                                'etihad'],\n",
    "                'VirginAtlantic': ['virgin atlantic',\n",
    "                                    'virginatlantic'],\n",
    "            }\n",
    "\n",
    "airlines_of_interest = ['AmericanAir', 'Other', 'British_Airways']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the counts of each category\n",
    "excluided = ['dm', 'customers', 'information', 'boarding', 'stuck']\n",
    "relevant_categories = [key for key in categories.keys() if key not in excluided]\n",
    "columns = ['total'] + relevant_categories\n",
    "\n",
    "base_path = Path('./pre-processed/')\n",
    "\n",
    "# create a dataframe with the counts of each category, restricted by timestamp_ms\n",
    "df = pd.DataFrame(columns=columns)\n",
    "for airline in airlines_of_interest:\n",
    "    df_airline = pd.read_csv(base_path / f'{airline}_category_counts.csv')\n",
    "    length = len(df_airline.index)\n",
    "    df_airline = df_airline[df_airline['timestamp_ms'] >= start_date_unix]\n",
    "    df_airline = df_airline[df_airline['timestamp_ms'] <= end_date_unix]\n",
    "    df_airline = df_airline.sum().to_frame().T\n",
    "    df_airline['total'] = length\n",
    "    df_airline['airline'] = airline\n",
    "    df = pd.concat([df, df_airline])\n",
    "\n",
    "\n",
    "df_category_counts = df.set_index('airline')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the dataframe into a table that shows both the counts, aswell as the keywords\n",
    "df_category_counts_table = df_category_counts.copy()\n",
    "df_category_counts_table = df_category_counts_table.drop(columns=['total'])\n",
    "df_category_counts_table = df_category_counts_table.transpose()\n",
    "df_category_counts_table = df_category_counts_table.reset_index()\n",
    "df_category_counts_table = df_category_counts_table.rename(columns={'index': 'category'})\n",
    "df_category_counts_table = df_category_counts_table.set_index('category')\n",
    "\n",
    "# add a column that displays the keywords for each category\n",
    "for category in relevant_categories:\n",
    "    keywords = categories[category]\n",
    "    df_category_counts_table.loc[category, 'keywords'] = \", \".join(keywords)\n",
    "\n",
    "df_category_counts_table\n",
    "\n",
    "# turn the dataframe into a table that shows the keywords per category\n",
    "df_keywords = pd.DataFrame(columns=['category', 'keywords'])\n",
    "for category in relevant_categories:\n",
    "    df_keywords.loc[len(df_keywords.index)] = [category, \", \".join(categories[category])]\n",
    "\n",
    "\n",
    "df_keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for the percentages\n",
    "df_category_percentages = pd.DataFrame(columns=relevant_categories)\n",
    "\n",
    "# create rows for each airline and initialize with NaN values\n",
    "for airline in airlines_of_interest:\n",
    "    df_category_percentages.loc[airline] = np.nan\n",
    "\n",
    "\n",
    "# calculate the percentage of each category, restricted to 2 decimal points\n",
    "for category in relevant_categories:\n",
    "    if category == 'total':\n",
    "        continue\n",
    "    for airline in airlines_of_interest:\n",
    "        raw_percentage = df_category_counts.loc[airline][category] / df_category_counts.loc[airline]['total'] * 100\n",
    "        percentage = round(raw_percentage, 2)\n",
    "        df_category_percentages.loc[airline, category] = percentage\n",
    "\n",
    "# Convert relevant columns to numeric type\n",
    "df_category_percentages[relevant_categories] = df_category_percentages[relevant_categories].apply(pd.to_numeric)\n",
    "\n",
    "\n",
    "# create a grouped bar chart \n",
    "# df_category_percentages.plot.bar(figsize=(20, 10), fontsize=20)\n",
    "# plt.title('Percentage of tweets per category', fontsize=30)\n",
    "# plt.xlabel('Airline', fontsize=20)\n",
    "# plt.ylabel('Percentage', fontsize=20)\n",
    "# plt.legend(fontsize=20)\n",
    "# plt.show()\n",
    "\n",
    "# create a grouped bar chart grouped by category\n",
    "df_category_percentages.transpose().plot.bar(figsize=(20, 10), fontsize=20)\n",
    "plt.title('Percentage of tweets per category per airline', fontsize=30)\n",
    "plt.xlabel('Category', fontsize=20)\n",
    "plt.ylabel('Percentage', fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment evolution in conversations (Oscar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f\"\"\" SELECT bs.bin_id, bs.cID, bs.bin_position, bs.break_airline, bs.sentiment_sum, bs.tweet_count, c.Tstart, c.Tend\n",
    "                    FROM binned_sentiment bs, conversations c\n",
    "                    WHERE bs.cID = c.id AND bs.break_airline NOT LIKE 'prev=%'\"\"\")\n",
    "binned_sentiment = cursor.fetchall()\n",
    "#all bins from conversations with more than 1 bin.\n",
    "\n",
    "query = f\"\"\" SELECT CASE WHEN bs1.break_airline='AmericanAir' THEN \"American Air\" \n",
    "                        WHEN bs1.break_airline = 'British_Airways' THEN \"British Airways\" \n",
    "                        ELSE \"Other Airlines\" END AS Airline, \n",
    "            CASE WHEN (bs1.sentiment_sum / bs1.tweet_count) > (bs2.sentiment_sum / bs2.tweet_count) THEN -1*ABS((bs1.sentiment_sum / bs1.tweet_count) - (bs2.sentiment_sum / bs2.tweet_count))\n",
    "            ELSE ABS((bs1.sentiment_sum / bs1.tweet_count) - (bs2.sentiment_sum / bs2.tweet_count))\n",
    "            END AS Average_sent_diff\n",
    "            FROM `binned_sentiment` bs1, `binned_sentiment` bs2\n",
    "            WHERE bs1.cID = bs2.cID AND bs2.bin_position = bs1.bin_position + 1 \n",
    "            AND bs1.cID IN (SELECT id\n",
    "                            FROM conversations\n",
    "                            WHERE Tstart > {start_date_unix} AND Tend < {end_date_unix})\"\"\"\n",
    "cursor.execute(query) #the airlines and average sentiment change per \n",
    "sentiment_change = cursor.fetchall()\n",
    "\n",
    "df_sentiment_change = pd.DataFrame(sentiment_change)\n",
    "df_sentiment_change.rename(columns ={0:\"Airline\",1:\"Sentiment Change [RoBERTa-XLM]\"}, inplace=True)\n",
    "df_sentiment_change[\"Sentiment Change [RoBERTa-XLM]\"] = df_sentiment_change[\"Sentiment Change [RoBERTa-XLM]\"].astype(float)\n",
    "#initialize a dataframe with the query data.\n",
    "\n",
    "sns.set_context(\"paper\", font_scale = 1.5)\n",
    "ax = sns.boxplot(data=df_sentiment_change,x='Airline',y='Sentiment Change [RoBERTa-XLM]', order= ['American Air','Other Airlines',\n",
    "                                                                                                  'British Airways'])\n",
    "ax.set_xlabel(ax.get_xlabel(), fontdict={'weight':'bold'})\n",
    "ax.set_ylabel(ax.get_ylabel(), fontdict={'weight':'bold'})\n",
    "ax.set_title('Average Sentiment Change during conversations of Airline',weight='bold'); #graph the data from the dataframe into 3 boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Change over reply-time per topic\n",
    "All functions found in processes.reply4_3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create dataframe from SQL query\n",
    "NOTE: make sure to have the right data-base.\n",
    "--> Expected running time: 20 sec\n",
    "\"\"\"\n",
    "df_tweets = reply_set_up()\n",
    "print(len(df_tweets.index))\n",
    "#df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only select the desired dates\n",
    "df_tweets = df_tweets[df_tweets['timestamp_ms'] >= start_date_unix]\n",
    "df_tweets = df_tweets[df_tweets['timestamp_ms'] <= end_date_unix]\n",
    "print(len(df_tweets.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert sentiment score to pos, neu, neg\"\"\"\n",
    "\n",
    "# Bin column sentiment1 (first bin) -> (pos=1, neu=0, neg=-1)\n",
    "df_tweets.loc[df_tweets['sentiment1'] < -0.2, 'sentiment1'] = -1\n",
    "df_tweets.loc[df_tweets['sentiment1'] > -0.2, 'sentiment1'] = 1\n",
    "df_tweets.loc[(df_tweets['sentiment1']  >= -0.2) & (df_tweets['sentiment1'] <= 0.2), 'sentiment1'] = 0\n",
    "\n",
    "# Bin column sentiment2 (second bin) -> (pos=1, neu=0, neg=-1)\n",
    "df_tweets.loc[df_tweets['sentiment2'] < -0.2, 'sentiment2'] = -1\n",
    "df_tweets.loc[df_tweets['sentiment2'] > -0.2, 'sentiment2'] = 1\n",
    "df_tweets.loc[(df_tweets['sentiment2']  >= -0.2) & (df_tweets['sentiment2'] <= 0.2), 'sentiment2'] = 0\n",
    "\n",
    "#df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between sentiment 1 and 2\n",
    "\n",
    "df_tweets['sentiment_change'] = df_tweets['sentiment2'] - df_tweets['sentiment1']\n",
    "#df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Clearing the data frames.\n",
    "\"\"\"\n",
    "\n",
    "# Creating a data frame per topic\n",
    "df_canceling = df_tweets[df_tweets['canceling'] != 0][['reply_time', 'sentiment_change']]\n",
    "df_boarding = df_tweets[df_tweets['boarding'] != 0][['reply_time', 'sentiment_change']]\n",
    "df_stuck = df_tweets[df_tweets['stuck'] != 0][['reply_time', 'sentiment_change']]\n",
    "df_booking = df_tweets[df_tweets['booking'] != 0][['reply_time', 'sentiment_change']]\n",
    "df_customers = df_tweets[df_tweets['customers'] != 0][['reply_time', 'sentiment_change']]\n",
    "df_dm = df_tweets[df_tweets['dm'] != 0][['reply_time', 'sentiment_change']]\n",
    "df_waiting = df_tweets[df_tweets['waiting'] != 0][['reply_time', 'sentiment_change']]\n",
    "df_money = df_tweets[df_tweets['money'] != 0][['reply_time', 'sentiment_change']]\n",
    "df_information = df_tweets[df_tweets['information'] != 0][['reply_time', 'sentiment_change']]\n",
    "df_staff = df_tweets[df_tweets['staff'] != 0][['reply_time', 'sentiment_change']]\n",
    "df_baggage = df_tweets[df_tweets['baggage'] != 0][['reply_time', 'sentiment_change']]\n",
    "\n",
    "\n",
    "# Binned of 1 min\n",
    "df_canceling_gro = df_canceling[['reply_time', 'sentiment_change']].groupby('reply_time').agg('mean').rolling(window=15).mean()\n",
    "df_boarding_gro = df_boarding[['reply_time', 'sentiment_change']].groupby('reply_time').agg('mean').rolling(window=15).mean()\n",
    "df_stuck_gro = df_stuck[['reply_time', 'sentiment_change']].groupby('reply_time').agg('mean').rolling(window=15).mean()\n",
    "df_booking_gro = df_booking[['reply_time', 'sentiment_change']].groupby('reply_time').agg('mean').rolling(window=15).mean()\n",
    "df_customers_gro = df_customers[['reply_time', 'sentiment_change']].groupby('reply_time').agg('mean').rolling(window=15).mean()\n",
    "df_dm_gro = df_dm[['reply_time', 'sentiment_change']].groupby('reply_time').agg('mean').rolling(window=15).mean()\n",
    "df_waiting_gro = df_waiting[['reply_time', 'sentiment_change']].groupby('reply_time').agg('mean').rolling(window=15).mean()\n",
    "df_money_gro = df_money[['reply_time', 'sentiment_change']].groupby('reply_time').agg('mean').rolling(window=15).mean()\n",
    "df_information_gro = df_information[['reply_time', 'sentiment_change']].groupby('reply_time').agg('mean').rolling(window=15).mean()\n",
    "df_staff_gro = df_staff[['reply_time', 'sentiment_change']].groupby('reply_time').agg('mean').rolling(window=15).mean()\n",
    "df_baggage_gro = df_baggage[['reply_time', 'sentiment_change']].groupby('reply_time').agg('mean').rolling(window=15).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number ineach category\n",
    "\n",
    "canceling_len = len(df_canceling.index)\n",
    "boarding_len = len(df_boarding.index)\n",
    "stuck_len = len(df_stuck.index)\n",
    "booking_len = len(df_booking.index)\n",
    "customers_len = len(df_customers.index)\n",
    "dm_len = len(df_dm.index)\n",
    "waiting_len = len(df_waiting.index)\n",
    "money_len = len(df_money.index)\n",
    "information_len = len(df_information.index)\n",
    "staff_len = len(df_staff.index)\n",
    "baggage_len = len(df_baggage.index)\n",
    "\n",
    "print(f\"\"\"\n",
    "      TOPIC COUNT\n",
    "      canceling: {canceling_len}\n",
    "      boarding: {boarding_len}\n",
    "      stuck: {stuck_len}\n",
    "      booking: {booking_len}\n",
    "      customers: {customers_len}\n",
    "      dm: {dm_len}\n",
    "      waiting: {waiting_len}\n",
    "      money: {money_len}\n",
    "      information: {information_len}\n",
    "      staff: {staff_len}\n",
    "      baggage: {baggage_len}\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get quantiles of the most relevant topics\n",
    "baggage_qua5 = df_baggage['reply_time'].astype(float).quantile([0, 0.25, 0.5, 0.75, 1])\n",
    "boarding_qua5 = df_boarding['reply_time'].astype(float).quantile([0, 0.25, 0.5, 0.75, 1])\n",
    "booking_qua5 = df_booking['reply_time'].astype(float).quantile([0, 0.25, 0.5, 0.75, 1])\n",
    "information_qua5 = df_information['reply_time'].astype(float).quantile([0, 0.25, 0.5, 0.75, 1])\n",
    "money_qua5 = df_money['reply_time'].astype(float).quantile([0, 0.25, 0.5, 0.75, 1])\n",
    "waiting_qua5 = df_waiting['reply_time'].astype(float).quantile([0, 0.25, 0.5, 0.75, 1])\n",
    "\n",
    "# Put quantiles values in a dataframe\n",
    "columnss=['baggage', 'boarding', 'booking', 'information', 'money', 'waiting']\n",
    "df_qua = pd.DataFrame()\n",
    "df_qua = pd.concat([df_qua, baggage_qua5])\n",
    "df_qua = pd.concat([df_qua, boarding_qua5], axis=1)\n",
    "df_qua = pd.concat([df_qua, booking_qua5], axis=1)\n",
    "df_qua = pd.concat([df_qua, information_qua5], axis=1)\n",
    "df_qua = pd.concat([df_qua, money_qua5], axis=1)\n",
    "df_qua = pd.concat([df_qua, waiting_qua5], axis=1)\n",
    "df_qua.columns = columnss\n",
    "df_qua\n",
    "#baggage_qua5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frames, split by quantiles, for every topic\n",
    "df_baggage_4 = split_df(df_baggage, baggage_qua5)\n",
    "df_boarding_4 = split_df(df_boarding, boarding_qua5)\n",
    "df_booking_4 = split_df(df_booking, booking_qua5)\n",
    "df_information_4 = split_df(df_information, information_qua5)\n",
    "df_money_4 = split_df(df_money, money_qua5)\n",
    "df_waiting_4 = split_df(df_waiting, waiting_qua5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate p-values for different quantiles and topics\n",
    "# Return a dataframe with all p-values\n",
    "p_baggage = welch_test(df_baggage_4, 'baggage')\n",
    "p_boarding = welch_test(df_boarding_4, 'boarding')\n",
    "p_booking = welch_test(df_booking_4, 'booking')\n",
    "p_information = welch_test(df_information_4, 'information')\n",
    "p_money = welch_test(df_money_4, 'money')\n",
    "p_waiting = welch_test(df_waiting_4, 'waiting')\n",
    "\n",
    "ds = [p_baggage, p_boarding, p_booking, p_information, p_money, p_waiting]\n",
    "\n",
    "df_p = pd.DataFrame(ds)\n",
    "df_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot only the 3 most relevant ones.\n",
    "\"\"\"\n",
    "# Plot the results in line-plot with marked quantiles\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, figsize=(5,7), sharey=True, sharex=True, layout=\"constrained\")\n",
    "plt.setp(ax, ylim=(-2,2))\n",
    "plt.setp(ax, xlim=(-2,720))\n",
    "\n",
    "\n",
    "# Booking\n",
    "sns.lineplot(data=df_booking_gro, x='reply_time', y='sentiment_change', ax=ax[0])\n",
    "ax[0].axvline(x = booking_qua5[0.25], c='purple', label='0.25')\n",
    "ax[0].axvline(x = booking_qua5[0.5], c='green', label='0.5')\n",
    "ax[0].axvline(x = booking_qua5[0.75], c='orange', label='0.75')\n",
    "ax[0].legend(title = 'Quartile')\n",
    "ax[0].set_title('Booking', size=20)\n",
    "ax[0].set_ylabel('Sentiment Change [XLM-RoBERTA]', size=12)\n",
    "\n",
    "# Money\n",
    "sns.lineplot(data=df_money_gro, x='reply_time', y='sentiment_change', ax=ax[1])\n",
    "ax[1].axvline(x = waiting_qua5[0.25], c='purple', label='0.25')\n",
    "ax[1].axvline(x = waiting_qua5[0.5], c='green', label='0.5')\n",
    "ax[1].axvline(x = waiting_qua5[0.75], c='orange', label='0.75')\n",
    "ax[1].legend(title = 'Quartile')\n",
    "ax[1].set_title('Money', size=20)\n",
    "ax[1].set_xlabel('Reply Time [min]', size=18)\n",
    "ax[1].set_ylabel('Sentiment Change [XLM-RoBERTA]', size=12)\n",
    "\n",
    "#fig.suptitle('Sentiment Change over Airline \\n Reply-Time in Conversations, Per Topic', weight='bold', size=20, y=1.10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results in line-plot with marked quantiles\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(15,7), sharey=True, sharex=True, layout=\"constrained\")\n",
    "plt.setp(ax, ylim=(-2,2))\n",
    "plt.setp(ax, xlim=(-2,720))\n",
    "\n",
    "\n",
    "# ROW 1\n",
    "# Baggage\n",
    "sns.lineplot(data=df_baggage_gro, x='reply_time', y='sentiment_change', ax=ax[0,0])\n",
    "ax[0,0].axvline(x = baggage_qua5[0.25], c='purple', label='0.25')\n",
    "ax[0,0].axvline(x = baggage_qua5[0.5], c='green', label='0.5')\n",
    "ax[0,0].axvline(x = baggage_qua5[0.75], c='orange', label='0.75')\n",
    "ax[0,0].legend(title = 'Quartile')\n",
    "ax[0,0].set_title('Baggage', size=20)\n",
    "ax[0,0].set_ylabel('Sentiment Change [XLM-RoBERTA]')\n",
    "\n",
    "# Boarding\n",
    "sns.lineplot(data=df_boarding_gro, x='reply_time', y='sentiment_change', ax=ax[0,1])\n",
    "ax[0,1].axvline(x = boarding_qua5[0.25], c='purple', label='0.25')\n",
    "ax[0,1].axvline(x = boarding_qua5[0.5], c='green', label='0.5')\n",
    "ax[0,1].axvline(x = boarding_qua5[0.75], c='orange', label='0.75')\n",
    "ax[0,1].legend(title = 'Quartile')\n",
    "ax[0,1].set_title('Boarding', size=20)\n",
    "\n",
    "# Booking\n",
    "sns.lineplot(data=df_booking_gro, x='reply_time', y='sentiment_change', ax=ax[0,2])\n",
    "ax[0,2].axvline(x = booking_qua5[0.25], c='purple', label='0.25')\n",
    "ax[0,2].axvline(x = booking_qua5[0.5], c='green', label='0.5')\n",
    "ax[0,2].axvline(x = booking_qua5[0.75], c='orange', label='0.75')\n",
    "ax[0,2].legend(title = 'Quartile')\n",
    "ax[0,2].set_title('Booking', size=20)\n",
    "\n",
    "# ROW 2\n",
    "# Information\n",
    "sns.lineplot(data=df_information_gro, x='reply_time', y='sentiment_change', ax=ax[1,0])\n",
    "ax[1,0].axvline(x = information_qua5[0.25], c='purple', label='0.25')\n",
    "ax[1,0].axvline(x = information_qua5[0.5], c='green', label='0.5')\n",
    "ax[1,0].axvline(x = information_qua5[0.75], c='orange', label='0.75')\n",
    "ax[1,0].legend(title = 'Quartile')\n",
    "ax[1,0].set_title('Information', size=20)\n",
    "ax[1,0].set_xlabel('Reply Time [min]', size=18)\n",
    "ax[1,0].set_ylabel('Sentiment Change [XLM-RoBERTA]')\n",
    "\n",
    "# Waiting\n",
    "sns.lineplot(data=df_waiting_gro, x='reply_time', y='sentiment_change', ax=ax[1,1])\n",
    "ax[1,1].axvline(x = money_qua5[0.25], c='purple', label='0.25')\n",
    "ax[1,1].axvline(x = money_qua5[0.5], c='green', label='0.5')\n",
    "ax[1,1].axvline(x = money_qua5[0.75], c='orange', label='0.75')\n",
    "ax[1,1].legend(title = 'Quartile')\n",
    "ax[1,1].set_xlabel('Reply Time [min]', size=18)\n",
    "ax[1,1].set_title('Waiting', size=20)\n",
    "\n",
    "# Money\n",
    "sns.lineplot(data=df_money_gro, x='reply_time', y='sentiment_change', ax=ax[1,2])\n",
    "ax[1,2].axvline(x = waiting_qua5[0.25], c='purple', label='0.25')\n",
    "ax[1,2].axvline(x = waiting_qua5[0.5], c='green', label='0.5')\n",
    "ax[1,2].axvline(x = waiting_qua5[0.75], c='orange', label='0.75')\n",
    "ax[1,2].legend(title = 'Quartile')\n",
    "ax[1,2].set_title('Money', size=20)\n",
    "ax[1,2].set_xlabel('Reply Time [min]', size=18)\n",
    "\n",
    "fig.suptitle('Sentiment Change over Airline Reply-Time in Conversations, Per Topic', weight='bold', size=26, y=1.07);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment per bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(\"./output/sentiment_per_bin.csv\")\n",
    "\n",
    "# for each position, get 75%, 50%, 25% percentile\n",
    "df = pd.read_csv(file_path)\n",
    "# drop all columns not within the time frame\n",
    "df = df[df[\"Tend\"] <= end_date_unix]\n",
    "df = df[df[\"Tstart\"] >= start_date_unix]\n",
    "\n",
    "# drop time columns\n",
    "df = df.drop(columns=[\"Tstart\", \"Tend\"])\n",
    "\n",
    "# drop the bin_id column\n",
    "df = df.drop(columns=[\"bin_id\"])\n",
    "\n",
    "# group the rows by bin_position\n",
    "df = df.groupby([\"bin_position\"]).agg(lambda x: list(x))\n",
    "\n",
    "df[\"75%\"] = df[\"sentiment\"].apply(lambda x: pd.Series(x).quantile(0.75))\n",
    "df[\"50%\"] = df[\"sentiment\"].apply(lambda x: pd.Series(x).quantile(0.50))\n",
    "df[\"25%\"] = df[\"sentiment\"].apply(lambda x: pd.Series(x).quantile(0.25))\n",
    "\n",
    "# show the lengths per position\n",
    "df[\"length\"] = df[\"sentiment\"].apply(lambda x: len(x))\n",
    "\n",
    "# find the first row that has a length of 5\n",
    "small_rows = df[df[\"length\"] <= 50]\n",
    "\n",
    "# get the index of the first row    \n",
    "first_small_row = small_rows.index[0]\n",
    "print(first_small_row)\n",
    "\n",
    "# drop all rows after the first small row\n",
    "df = df.drop(df.index[(first_small_row - 1):])\n",
    "df = df.drop(columns=[\"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the line graph, coloring the area between 75% and 25%\n",
    "ax = df.plot(y=\"50%\", figsize=(15, 10), legend=False)\n",
    "ax.fill_between(df.index, df[\"75%\"], df[\"25%\"], alpha=0.2)\n",
    "ax.set_xlabel(\"Position\")\n",
    "ax.set_ylabel(\"Sentiment\")\n",
    "ax.set_title(\"Sentiment over time\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple regression functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQL database\n",
    "\n",
    "connection=database.connect.getConnection()\n",
    "cursor=connection.cursor(buffered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell contains all functions broken up to query all relevant variables into dataframes \n",
    "#all conversations queried involved airlines and have length of at least 3\n",
    "\n",
    "def avg_resp_time():\n",
    "\n",
    "    # creates a dataframe with conversation Id and avg reply time of the airlne throughout the conversation\n",
    "\n",
    "    Query= (f\"\"\"\n",
    "                SELECT part_of.cID, AVG(tweets.reply_time) \n",
    "                FROM tweets, part_of, conversations \n",
    "                WHERE tweets.id = part_of.tID AND part_of.cID = conversations.id AND conversations.Airline <> '0' \n",
    "                GROUP BY conversations.id\n",
    "                \"\"\")\n",
    "    execute=cursor.execute(Query)\n",
    "    result=cursor.fetchall()\n",
    "    df1 = pd.DataFrame(result)\n",
    "    df1.columns = ['cID','reply_time']\n",
    "    df1 = df1.set_index('cID')\n",
    "    return df1\n",
    "\n",
    "\n",
    "def init_sent():\n",
    "\n",
    "    # Finds the initial sentiment and conversation length \n",
    "     try:\n",
    "        Query = (f\"\"\"\n",
    "                SELECT conversations.id, sentiment, conversations.Length, conversations.Tstart, conversations.Tend\n",
    "                FROM conversations , part_of ,tweets\n",
    "                WHERE conversations.ID = part_of.cID AND tweets.id=part_of.tID AND Position = 1 and conversations.Airline <>'0';\n",
    "\n",
    "                 \"\"\")\n",
    "        execute = cursor.execute(Query)\n",
    "        result = cursor.fetchall()\n",
    "        df2 = pd.DataFrame(result)\n",
    "        df2.columns = ['cID','init_sentiment','Length','Tstart','Tend']\n",
    "        df2 = df2.set_index('cID')\n",
    "        return df2\n",
    "\n",
    "     except:\n",
    "        print(\"Error\")\n",
    "        return 0.00\n",
    "\n",
    "\n",
    "def sent_change():\n",
    "\n",
    "    # Finds the avg sentiment change per conversation\n",
    "    try:\n",
    "        Query = (f\"\"\"\n",
    "                 SELECT bs1.cID, CASE\n",
    "\t            WHEN (bs1.sentiment_sum/bs1.tweet_count) > (bs2.sentiment_sum / bs2.tweet_count) THEN -1*ABS((bs1.sentiment_sum / bs1.tweet_count) - (bs2.sentiment_sum / bs2.tweet_count))\n",
    "                 ELSE ABS((bs1.sentiment_sum / bs1.tweet_count) - (bs2.sentiment_sum / bs2.tweet_count))\n",
    "                END AS average_sent_diff\n",
    "                    FROM `binned_sentiment` bs1, `binned_sentiment` bs2, (SELECT cID, MAX(bin_position) AS max_pos FROM `binned_sentiment` GROUP BY cID) AS max_bins WHERE bs1.cID = bs2.cID AND bs1.cID = max_bins.cID AND bs1.bin_position= 1 AND bs2.bin_position=max_pos\n",
    "                \n",
    "                    \"\"\")\n",
    "        execute = cursor.execute(Query)\n",
    "        result = cursor.fetchall()\n",
    "        \n",
    "        df3 = pd.DataFrame(result)\n",
    "        df3.columns = ['cID','avg_sent_change']\n",
    "        df3 = df3.set_index('cID')\n",
    "        return df3\n",
    "\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "        return 0.00\n",
    "\n",
    "def get_conversation_categories():\n",
    "    excluided = ['dm', 'customers', 'information']\n",
    "    relevant_categories = [key for key in categories.keys() if key not in excluided]\n",
    "    # for each conversation, get weather it incluides a category.\n",
    "    # returns a dataframe with the conversation id and the categorys\n",
    "    categories_query = \"\"\n",
    "\n",
    "    for category in relevant_categories:\n",
    "        categories_query += f\"(CASE WHEN tweets.{category} != 0 THEN 1 ELSE 0 END) AS {category}, \"\n",
    "\n",
    "    query = (f\"\"\"\n",
    "                SELECT part_of.cID, {categories_query[:-2]}\n",
    "                FROM tweets, part_of\n",
    "                WHERE tweets.id = part_of.tID \n",
    "                GROUP BY part_of.cID\n",
    "                \"\"\")\n",
    "    execute = cursor.execute(query)\n",
    "    result = cursor.fetchall()\n",
    "    df4 = pd.DataFrame(result)\n",
    "    df4.columns = ['cID'] + relevant_categories\n",
    "    # remove the index column\n",
    "    df4 = df4.set_index('cID')\n",
    "    return df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates table\n",
    "df_avg_resp_time=avg_resp_time()\n",
    "df_sent_change=sent_change()\n",
    "df_init_sent=init_sent()\n",
    "df_convo_cat=get_conversation_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the data frames on the shared index 'cID'\n",
    "joined_df = pd.merge(df_avg_resp_time, df_sent_change, on='cID')\n",
    "joined_df = pd.merge(joined_df, df_init_sent, on='cID')\n",
    "joined_df = pd.merge(joined_df, df_convo_cat, on='cID')\n",
    "joined_df=joined_df.apply(pd.to_numeric, errors='coerce')\n",
    "#pick out datetime\n",
    "joined_df = joined_df[(joined_df['Tstart'] >= start_date_unix) & (joined_df['Tend'] <= end_date_unix)]\n",
    "#remove date time\n",
    "joined_df = joined_df.drop(['Tstart', 'Tend'], axis=1)\n",
    "\n",
    "# Print the joined data frame to check\n",
    "print(joined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_matrix():\n",
    "    # function takes in dataframe and creates a correlation matrix\n",
    "    df =joined_df\n",
    "\n",
    "    df = df.iloc[:, 1:-1]\n",
    "\n",
    "    corr = df.corr(method='spearman')\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr, dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True, sep=100)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0, linewidths=.5)\n",
    "\n",
    "    fig.suptitle('Correlation matrix of features', fontsize=15,fontweight='bold')\n",
    "\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "corr_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_regress():\n",
    "    #function takes in dataframe and returns all coeffients and r^2 of the fit model.\n",
    "    df = joined_df\n",
    "\n",
    "    features = ['reply_time', 'init_sentiment', 'Length', 'booking','canceling','money','baggage','staff','waiting','boarding','stuck']\n",
    "    target = 'avg_sent_change'\n",
    "\n",
    "    X = df[features].values.reshape(-1, len(features))\n",
    "    y = df[target].values\n",
    "\n",
    "    ols = linear_model.LinearRegression()\n",
    "    model = ols.fit(X, y)\n",
    "\n",
    "    #model.coef_\n",
    "\n",
    "    print('Features                :  %s' % features)\n",
    "    print('Regression Coefficients : ', [round(item, 7) for item in model.coef_])\n",
    "    print('R-squared               :  %.2f' % model.score(X, y))\n",
    "    print('Y-intercept             :  %.2f' % model.intercept_)\n",
    "    print('')\n",
    "multiple_regress()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbl-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
